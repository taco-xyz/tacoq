
> [!WARNING]
> This section assumes you have a basic understanding of the core concepts of
> task queues and TacoQ. Read the [Core Concepts](/quickstart/core-concepts)
> section if you haven't already.

# Prerequisites

- [Docker](https://docs.docker.com/get-docker/)
- [Python](https://www.python.org/downloads/)

> [!TIP]
> We recommend using [UV](https://docs.astral.sh/uv/getting-started/installation/)
> to run Python projects.

# Infrastructure

TacoQ requires the Postgres, RabbitMQ, and the Relay to be running. Let's start
by creating a `docker-compose.yml` file to launch them:

```yml
volumes:
  rabbitmq_data: {}
  postgres_data: {}

services:

  # ================================================
  # TacoQ Relay
  # The relay has two functions:
  # 1. Reads task updates from the message broker
  #    and writes them to the database.
  # 2. Has a REST API for getting tasks by ID.
  # ================================================

  relay:
    image: ghcr.io/taco-xyz/tacoq-relay:latest
    ports:
      - "3000:3000"
    depends_on:
      rabbitmq:
        condition: service_healthy
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/health"]
      interval: 5s
      timeout: 5s
      retries: 5
    environment:
      DATABASE_URL: postgresql://user:password@localhost:5432/tacoq
      TACOQ_DATABASE_READER_URL: postgresql://user:password@localhost:5432/tacoq
      TACOQ_DATABASE_WRITER_URL: postgresql://user:password@localhost:5432/tacoq
      TACOQ_BROKER_ADDR: amqp://user:password@localhost:5672
      OTEL_EXPORTER_OTLP_TRACES_ENDPOINT: http://localhost:4317
      OTEL_EXPORTER_OTLP_TRACES_PROTOCOL: grpc
      OTEL_TRACES_SAMPLER: always_on
      OTEL_SERVICE_NAME: tacoq.manager

  # ================================================
  # Broker
  # This is the message broker where all tasks get
  # routed through to the appropriate worker and to
  # the relay so it can save them to the database.
  # ================================================

  rabbitmq:
    image: rabbitmq:4-management
    ports:
      - "5672:5672"
      - "15672:15672"
    environment:
      RABBITMQ_DEFAULT_USER: user
      RABBITMQ_DEFAULT_PASS: password
    volumes:
      - rabbitmq_data:/var/lib/rabbitmq
    healthcheck:
      test: ["CMD", "rabbitmq-diagnostics", "check_port_connectivity"]
      interval: 5s
      timeout: 5s
      retries: 5

  # ================================================
  # Storage
  # This is the database where all tasks get saved.
  # ================================================

  postgres:
    image: postgres:latest
    environment:
      POSTGRES_USER: user
      POSTGRES_PASSWORD: password
      POSTGRES_DB: tacoq
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U user -d tacoq"]
      interval: 5s
      timeout: 5s
      retries: 5

```

Now just run `docker compose up` to start the services.

# Client

## Worker

Now that the infrastructure is running, we want to create a worker that can
execute tasks. Let's start by installing the TacoQ Python SDK:

```bash
pip install tacoq
```

The worker must know how to contact the relay and the message broker, so we must
create a configuration object:

```python
from tacoq import (
    WorkerApplication,
    BrokerConfig,
    WorkerApplicationConfig,
)

broker_config = BrokerConfig(url="amqp://user:password@localhost:5672")
worker_config = WorkerApplicationConfig(
    name="worker_waiter_1",
    kind="worker_waiter_kind",
    broker_config=broker_config,
    broker_prefetch_count=5,
)

worker_app = WorkerApplication(config=worker_config)
```

Note the worker field `kind` is set to `"worker_waiter_kind"`. This field will be
used by the publisher to know to which set of workers to send the task. We recommend
using environment variables to coordinate these values.

The worker application has been created, but it doesn't know how to handle any
tasks that come its way. So let's declare a task:

```python
import json
from typing import Any
from tacoq import TaskInput, TaskOutput

@worker_app.task(kind="task_wait_n_seconds")
async def task_wait_n_seconds(input_data: TaskInput) -> TaskOutput:
    input_data_dict: dict[str, Any] = json.loads(input_data)
    seconds = input_data_dict.get("seconds", 0)
    await asyncio.sleep(seconds)

    return json.dumps(
        {
            "result": "Hello, world! You waited for %d seconds" % seconds,
            "seconds": seconds,
        }
    )
```

Again, note the task field `kind` is set to `"task_wait_n_seconds"`. You can think about it
the following way:
- Worker Kind: Helps the publisher know which set of workers to send the task to.
- Task Kind: Helps the worker know which method to use to handle a task.

Now that we have a worker and a task, we can start up the worker:

```python
import asyncio
if __name__ == "__main__":
    asyncio.run(worker_app.entrypoint())
```

The worker is ready to handle tasks. Now let's publish some tasks for it to handle.

## Publisher

Let's start by setting up the publisher and its configuration:

```python
from tacoq import PublisherClient, BrokerConfig, RelayConfig, Task

broker_config = BrokerConfig(url="amqp://user:password@localhost:5672")
relay_config = RelayConfig(url="http://localhost:3000")
publisher = PublisherClient(broker_config=broker_config, relay_config=relay_config)
```

With the publisher application created, we don't need to run an entrypoint.
Instead, we will simply use to when we want to publish or retrieve a task.

Let's publish a new task:

```python
# We must serialize the input data in a string so that it can be passed and 
# interpreted by the worker!
task_input = json.dumps({"duration": 2}) 

# The task is published to the message broker. Note that the worker kind and
# task kind but be properly configured and match the worker and task kinds
# in the worker application.
task = await publisher.publish_task(
    worker_kind="worker_waiter_kind",
    task_kind="task_wait_n_seconds",
    input_data=task_input,
)

# We can now fetch retrieve the task's status and results. You can optionally
# set `retry_until_complete` to `True` to have the publisher retry the request
# until the task has been completed by the worker.
completed_task = await publisher.get_task(task.id, retry_until_complete=True)

# Let's load the results into a dictionary and print them.
result = json.loads(completed_task.results)
print(result)

# Hurray!
```

Congratulations! You've just published and retrieved a task using TacoQ.
