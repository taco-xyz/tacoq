{
  "children": [
    {
      "metadata": {
        "title": "Quickstart",
        "description": "Get started using TacoQ",
        "icon": "Rocket",
        "index": 0
      },
      "children": [
        {
          "url": "/quickstart/core-concepts",
          "metadata": {
            "title": "Core Concepts",
            "description": "Understand the core concepts of task queues, TacoQ, and how everything fits together at a basic level.",
            "icon": "GraduationCap",
            "index": 0
          },
          "rawContent": "export const metadata = {\r\n  title: \"Core Concepts\",\r\n  description:\r\n    \"Understand the core concepts of task queues, TacoQ, and how everything fits together at a basic level.\",\r\n  keywords: [\r\n    \"task queue\",\r\n    \"asynchronous execution\",\r\n    \"worker_kind\",\r\n    \"task_kind\",\r\n    \"task priority\",\r\n    \"task status\",\r\n    \"task ID\",\r\n    \"message broker\",\r\n    \"RabbitMQ\",\r\n    \"workers\",\r\n    \"Postgres\",\r\n    \"publishers\",\r\n    \"relay\",\r\n    \"REST API\",\r\n    \"task publishing\",\r\n    \"task results\",\r\n    \"horizontal scaling\",\r\n    \"multi-language\",\r\n    \"interoperability\",\r\n    \"Python SDK\",\r\n    \"Rust\",\r\n    \"JavaScript\",\r\n    \"async functions\",\r\n    \"pydantic\",\r\n    \"type safety\",\r\n    \"hot reloading\",\r\n    \"developer experience\",\r\n    \"observability\",\r\n    \"system architecture\",\r\n    \"task routing\",\r\n    \"task execution\",\r\n    \"database integration\",\r\n    \"service integration\",\r\n  ],\r\n  openGraph: {\r\n    type: \"website\",\r\n    locale: \"en_US\",\r\n    siteName: \"TacoQ Docs\",\r\n    title: \"Core Concepts | TacoQ Docs\",\r\n    description:\r\n      \"Understand the core concepts of task queues, TacoQ, and how everything fits together at a basic level.\",\r\n  },\r\n};\r\n\r\n# What is TacoQ?\r\n\r\nTacoQ is a task queue system that allows you to schedule tasks to be executed\r\nasynchronously in workers outside your application. (e.g. in a different container).\r\n\r\n# Core Concepts: Rapid-fire Overview\r\n\r\nTo properly use TacoQ, it is recommended that you have a basic understanding\r\nof the core concepts of task queues. This section will provide a rapid-fire\r\noverview of the most important topics so you can hit the ground running.\r\n\r\n## Tasks\r\n\r\nTasks are a unit of work that can be scheduled to be executed asynchronously.\r\nHere are some core properties of tasks that you should know about:\r\n\r\n- Tasks have a `worker_kind` and a `task_kind`, which are used to identify\r\n  which set of workers should execute each task (respectively). If you are\r\n  familiar with message queues, you can think of them as routing keys.\r\n- Tasks have a `priority` value, which is used to sort tasks and determine which\r\n  ones to assign to workers first. Once a task reaches a worker, its priority\r\n  will no longer matter and it will be executed no matter what its priority is.\r\n- When generated, tasks get an `id`, which is a unique identifier for the task\r\n  that can be later used to retrieve the task's status and results.\r\n- Tasks have a `status` that can be `PENDING`, `RUNNING`, or `COMPLETED`. If\r\n  the task has been completed, the `is_error` field will be set to either `true`\r\n  or `false` depending on the success of the task.\r\n\r\n## Message Broker\r\n\r\nThe message broker carries task assignments, updates, and results across the\r\nentire TacoQ system. The broker is also responsible for routing the tasks to the\r\ncorrect set of workers based on its properties.\r\n\r\n<Note>\r\n  TacoQ uses [RabbitMQ](https://www.rabbitmq.com/) as its message broker.\r\n</Note>\r\n\r\n## Workers\r\n\r\nWorkers receive task assignments from the message broker, execute them, and then\r\nsend the result back through the message broker so they can be stored in the database.\r\n\r\n<Note>\r\nTacoQ uses [Postgres](https://www.postgresql.org/) as its database.\r\n</Note>\r\n\r\n## Publishers\r\n\r\nPublishers are services that publish tasks to the message broker.\r\n\r\n<Tip>\r\n  The publisher does not need to be a dedicated application. Instead, any\r\n  service can embed the publisher in its code to publish tasks. (even workers!).\r\n</Tip>\r\n\r\n## Relay\r\n\r\nThe relay is a unique concept to TacoQ. It is a service that acts as TacoQ's\r\nengine, and has the following characteristics:\r\n\r\n- Hosts a REST API that enables any SDK or even non-supported languages to\r\n  retrieve a task's result and status. Task publishing is done through the\r\n  message broker when possible, but the relay also allows for task publishing\r\n  for languages without an SDK.\r\n- Acts as a permanent consumer of messages from the broker to update the\r\n  database with the latest task statuses and results.\r\n- Is horizontally scalable and can be replicated to run thousands of instances.\r\n\r\n# What makes TacoQ different?\r\n\r\nTacoQ aims to differentiate itself from other task queues by providing the\r\nfollowing features:\r\n\r\n- Multi-language interoperability (e.g. schedule a CPU-intensive task to be\r\n  executed by a Rust worker from a Python application, generate a cute PDF\r\n  in a Javascript worker from a Rust API).\r\n- Modern SDKs for popular languages like Python, supporting async functions,\r\n  `pydantic`, having a type safe API, built-in hot reloading, and more.\r\n- Integration via REST API, allowing languages without a dedicated SDK to\r\n  easily schedule and retrieve task results.\r\n- Great developer experience with a focus on programmer ergonomics, good\r\n  documentation, and explainability (for example, with built-in observability).\r\n\r\n<Tip>\r\n  You can learn more about TacoQ's architecture and design in the [System\r\n  Architecture](/technical-reference/system-architecture) section.\r\n</Tip>\r\n",
          "headers": [
            {
              "title": "What is TacoQ?",
              "type": "h1"
            },
            {
              "title": "Core Concepts: Rapid-fire Overview",
              "type": "h1"
            },
            {
              "title": "Tasks",
              "type": "h2"
            },
            {
              "title": "Message Broker",
              "type": "h2"
            },
            {
              "title": "Workers",
              "type": "h2"
            },
            {
              "title": "Publishers",
              "type": "h2"
            },
            {
              "title": "Relay",
              "type": "h2"
            },
            {
              "title": "What makes TacoQ different?",
              "type": "h1"
            }
          ]
        },
        {
          "url": "/quickstart/setup",
          "metadata": {
            "title": "Setup",
            "description": "Get TacoQ up and running on your project using Docker and the Python SDK.",
            "icon": "ArrowDownToLine",
            "index": 1
          },
          "rawContent": "export const metadata = {\r\n  title: \"Setup\",\r\n  description:\r\n    \"Get TacoQ up and running on your project using Docker and the Python SDK.\",\r\n  keywords: [\r\n    \"Docker setup\",\r\n    \"Python SDK\",\r\n    \"TacoQ installation\",\r\n    \"RabbitMQ setup\",\r\n    \"Postgres setup\",\r\n    \"TacoQ Relay\",\r\n    \"docker-compose\",\r\n    \"infrastructure setup\",\r\n    \"worker configuration\",\r\n    \"broker configuration\",\r\n    \"task handling\",\r\n    \"PublisherClient\",\r\n    \"RelayClient\",\r\n    \"task publishing\",\r\n    \"task execution\",\r\n    \"task retrieval\",\r\n    \"worker_kind\",\r\n    \"task_kind\",\r\n    \"async tasks\",\r\n    \"message broker\",\r\n    \"database integration\",\r\n    \"REST API\",\r\n    \"health checks\",\r\n    \"environment variables\",\r\n    \"task serialization\",\r\n    \"JSON handling\",\r\n    \"task results\",\r\n    \"worker application\",\r\n    \"UV package manager\",\r\n    \"task status\",\r\n    \"broker prefetch\",\r\n    \"task input\",\r\n    \"task output\",\r\n    \"async/await\",\r\n    \"Docker volumes\",\r\n    \"service dependencies\",\r\n  ],\r\n  openGraph: {\r\n    type: \"website\",\r\n    locale: \"en_US\",\r\n    siteName: \"TacoQ Docs\",\r\n    title: \"Setup | TacoQ Docs\",\r\n    description:\r\n      \"Get TacoQ up and running on your project using Docker and the Python SDK.\",\r\n  },\r\n};\r\n\r\n<Warning>\r\n  This section assumes you have a basic understanding of the core concepts of\r\n  task queues and TacoQ. Read the [Core Concepts](/quickstart/core-concepts)\r\n  section if you haven't already.\r\n</Warning>\r\n\r\n# Prerequisites\r\n\r\n- [Docker](https://docs.docker.com/get-docker/)\r\n- [Python](https://www.python.org/downloads/)\r\n\r\n<Tip>\r\n  We recommend using\r\n  [UV](https://docs.astral.sh/uv/getting-started/installation/) to run Python\r\n  projects.\r\n</Tip>\r\n\r\n# Infrastructure\r\n\r\nTacoQ requires Postgres, RabbitMQ, and the Relay to be running. Let's start\r\nby creating a `docker-compose.yml` file to launch them:\r\n\r\n```yml\r\nvolumes:\r\n  rabbitmq_data: {}\r\n  postgres_data: {}\r\n\r\nservices:\r\n  # ================================================\r\n  # TacoQ Relay\r\n  # The relay has two functions:\r\n  # 1. Reads task updates from the message broker\r\n  #    and writes them to the database.\r\n  # 2. Has a REST API for getting tasks by ID.\r\n  # ================================================\r\n\r\n  relay:\r\n    image: ghcr.io/taco-xyz/tacoq-relay:latest\r\n    ports:\r\n      - \"3000:3000\"\r\n    depends_on:\r\n      rabbitmq:\r\n        condition: service_healthy\r\n      postgres:\r\n        condition: service_healthy\r\n    healthcheck:\r\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:3000/health\"]\r\n      interval: 5s\r\n      timeout: 5s\r\n      retries: 5\r\n    environment:\r\n      TACOQ_DATABASE_URL: postgresql://user:password@localhost:5432/tacoq\r\n      TACOQ_BROKER_URL: amqp://user:password@localhost:5672\r\n\r\n  # ================================================\r\n  # Broker\r\n  # This is the message broker where all tasks get\r\n  # routed through to the appropriate worker and to\r\n  # the relay so it can save them to the database.\r\n  # ================================================\r\n\r\n  rabbitmq:\r\n    image: rabbitmq:4-management\r\n    ports:\r\n      - \"5672:5672\"\r\n      - \"15672:15672\"\r\n    environment:\r\n      RABBITMQ_DEFAULT_USER: user\r\n      RABBITMQ_DEFAULT_PASS: password\r\n    volumes:\r\n      - rabbitmq_data:/var/lib/rabbitmq\r\n    healthcheck:\r\n      test: [\"CMD\", \"rabbitmq-diagnostics\", \"check_port_connectivity\"]\r\n      interval: 5s\r\n      timeout: 5s\r\n      retries: 5\r\n\r\n  # ================================================\r\n  # Storage\r\n  # This is the database where all tasks get saved.\r\n  # ================================================\r\n\r\n  postgres:\r\n    image: postgres:latest\r\n    environment:\r\n      POSTGRES_USER: user\r\n      POSTGRES_PASSWORD: password\r\n      POSTGRES_DB: tacoq\r\n    ports:\r\n      - \"5432:5432\"\r\n    volumes:\r\n      - postgres_data:/var/lib/postgresql/data\r\n    healthcheck:\r\n      test: [\"CMD-SHELL\", \"pg_isready -U user -d tacoq\"]\r\n      interval: 5s\r\n      timeout: 5s\r\n      retries: 5\r\n```\r\n\r\nRun `docker compose up` to start the services and we're ready to go!\r\n\r\n# Client\r\n\r\n## Worker\r\n\r\nWith the infrastructure running, we want to create a worker that can\r\nexecute tasks. Let's start by installing the TacoQ Python SDK:\r\n\r\n```bash\r\npip install tacoq\r\n```\r\n\r\nor, for UV users:\r\n\r\n```bash\r\nuv init\r\nuv add tacoq\r\n```\r\n\r\nThe worker must know how to receive new task assignments and send updates\r\nthrough the broker, so let's start by configuring that:\r\n\r\n```python\r\nfrom tacoq import (\r\n    WorkerApplication,\r\n    BrokerConfig,\r\n    WorkerApplicationConfig,\r\n)\r\n\r\nbroker_config = BrokerConfig(url=\"amqp://user:password@localhost:5672\")\r\nworker_config = WorkerApplicationConfig(\r\n    name=\"worker_waiter_1\",\r\n    kind=\"worker_waiter_kind\",\r\n    broker_config=broker_config,\r\n    broker_prefetch_count=5,\r\n)\r\n\r\nworker_app = WorkerApplication(config=worker_config)\r\n```\r\n\r\n<Note>\r\n  Note that the worker field `kind` is set to `\"worker_waiter_kind\"`. This field\r\n  will be used by the publisher to know which set of workers to send the task\r\n  to. We recommend using environment variables to align these values.\r\n</Note>\r\n\r\nThe worker application has been created, but it doesn't know how to handle any\r\ntasks that come its way. So, let's teach it to handle a task:\r\n\r\n```python\r\nimport json\r\nfrom tacoq import TaskInput, TaskOutput\r\n\r\n@worker_app.task(kind=\"task_wait_n_seconds\")\r\nasync def task_wait_n_seconds(input_data: TaskInput) -> TaskOutput:\r\n\r\n    # The input data must be de-serialized from a string into your preferred\r\n    # data structure. In this case, we're using JSON. You could use Avro or Proto!\r\n    input_data_dict: dict[str, Any] = json.loads(input_data)\r\n    seconds = input_data_dict.get(\"seconds\", 0)\r\n\r\n    # The task is now executed. Here we simply wait for the specified number of\r\n    # seconds and then return a results dictionary.\r\n    await asyncio.sleep(seconds)\r\n\r\n    # The results are serialized back into a string so that they can be\r\n    # transported back to whomever requested the task.\r\n    return json.dumps(\r\n        {\r\n            \"result\": \"Hello, world! You waited for %d seconds\" % seconds,\r\n            \"seconds\": seconds,\r\n        }\r\n    )\r\n```\r\n\r\n<Note>\r\n  Note the task field `kind` is set to `\"task_wait_n_seconds\"`. You can think\r\n  about it the following way: - **Worker Kind**: Helps the publisher know which\r\n  set of workers to send the task to. - **Task Kind**: Helps the worker know\r\n  which method to use to handle a task. If you're familiar with task queues,\r\n  you're probably used to only specifying the task kind and not the worker kind.\r\n  You can read about this design decision in the [System\r\n  Architecture](/technical-reference/system-architecture#h2-worker) section.\r\n</Note>\r\n\r\nNow that our worker is ready to handle tasks, we can boot it up via its\r\n`entrypoint` method:\r\n\r\n```python\r\nif __name__ == \"__main__\":\r\n    import asyncio\r\n    asyncio.run(worker_app.entrypoint())\r\n```\r\n\r\nThe worker is running and ready to handle tasks. Now, let's publish some tasks\r\nfor it to take care of!\r\n\r\n## PublisherClient\r\n\r\nWe'll start by setting up the publisher and its configuration. The publisher's\r\none and only responsability is to publish tasks via the message broker so that\r\nthe relay and the worker can take care of the rest.\r\n\r\n```python\r\nfrom tacoq import PublisherClient, BrokerConfig\r\n\r\nbroker_config = BrokerConfig(url=\"amqp://user:password@localhost:5672\")\r\npublisher = PublisherClient(broker_config=broker_config)\r\n```\r\n\r\nWith the publisher application created, we don't need to run an entrypoint.\r\nInstead, we will simply use the publisher to publish the task.\r\n\r\nLet's publish a new task, wait for it to complete, and then retrieve the results:\r\n\r\n```python\r\n# We must serialize the input data in a string so that it can be passed and\r\n# interpreted by the worker!\r\ntask_input = json.dumps({\"duration\": 2})\r\n\r\n# The task is published to the message broker. Note that the worker kind and\r\n# task kind but be properly configured and match the worker and task kinds\r\n# in the worker application.\r\ntask = await publisher.publish_task(\r\n    worker_kind=\"worker_waiter_kind\",\r\n    task_kind=\"task_wait_n_seconds\",\r\n    input_data=task_input,\r\n)\r\n\r\n# The task's ID is important for later!\r\ntask_id = task.id\r\n\r\n```\r\n\r\nOur task has now been published and is being worked on. But how do we retrieve\r\nthe task's status and results?\r\n\r\n## RelayClient\r\n\r\nWhen the worker is done with the task, it sends the results to the relay, who\r\nsaves them in the database. The relay can be queried via REST for the task's\r\ncurrent state.\r\n\r\nTo communicate with the relay, we can use the `RelayClient` class:\r\n\r\n```python\r\nfrom tacoq import RelayClient\r\n\r\n# The relay's URL is passed as an argument to the constructor.\r\nrelay_client = RelayClient(url=\"http://localhost:3000\")\r\n\r\n# We can now fetch retrieve the task's status and results. You can optionally\r\n# set `retry_until_complete` to `True` to have the publisher retry the request\r\n# until the task has been completed by the worker.\r\ncompleted_task = await relay_client.get_task(task_id)\r\n\r\n# Let's load the results into a dictionary and print them.\r\nresult = json.loads(completed_task.results)\r\nprint(result)\r\n\r\n# Hurray!\r\n```\r\n\r\nCongratulations! You've just published, executed, and retrieved a task using\r\nTacoQ. You can keep learning more about TacoQ in the [Technical Reference](/technical-reference)\r\nsection.\r\n",
          "headers": [
            {
              "title": "Prerequisites",
              "type": "h1"
            },
            {
              "title": "Infrastructure",
              "type": "h1"
            },
            {
              "title": "Client",
              "type": "h1"
            },
            {
              "title": "Worker",
              "type": "h2"
            },
            {
              "title": "PublisherClient",
              "type": "h2"
            },
            {
              "title": "RelayClient",
              "type": "h2"
            }
          ]
        }
      ]
    },
    {
      "metadata": {
        "title": "Technical Reference",
        "description": "Technical details about TacoQ.",
        "icon": "BookOpen",
        "index": 1
      },
      "children": [
        {
          "url": "/technical-reference/system-architecture",
          "metadata": {
            "title": "System Architecture",
            "description": "Learn how services interact with each other and why they are structured the way they are.",
            "icon": "Building2",
            "index": 1
          },
          "rawContent": "export const metadata = {\r\n  title: \"System Architecture\",\r\n  description:\r\n    \"Learn how services interact with each other and why they are structured the way they are.\",\r\n  keywords: [\r\n    \"system architecture\",\r\n    \"broker architecture\",\r\n    \"RabbitMQ\",\r\n    \"relay_queue\",\r\n    \"task_exchange\",\r\n    \"queue durability\",\r\n    \"task routing\",\r\n    \"task priority\",\r\n    \"Postgres database\",\r\n    \"sqlx\",\r\n    \"database architecture\",\r\n    \"relay service\",\r\n    \"task updates\",\r\n    \"REST API\",\r\n    \"Axum framework\",\r\n    \"horizontal scaling\",\r\n    \"task cleanup\",\r\n    \"worker architecture\",\r\n    \"worker_kind\",\r\n    \"task_kind\",\r\n    \"publisher client\",\r\n    \"relay client\",\r\n    \"message broker\",\r\n    \"task storage\",\r\n    \"service interaction\",\r\n    \"queue configuration\",\r\n    \"LISTEN NOTIFY\",\r\n    \"task TTL\",\r\n    \"load balancing\",\r\n    \"service replication\",\r\n    \"task execution\",\r\n    \"system services\",\r\n    \"user services\",\r\n    \"task routing strategy\",\r\n    \"service scalability\",\r\n    \"system observability\",\r\n    \"OTEL tracing\",\r\n  ],\r\n  openGraph: {\r\n    type: \"website\",\r\n    locale: \"en_US\",\r\n    siteName: \"TacoQ Docs\",\r\n    title: \"System Architecture | TacoQ Docs\",\r\n    description:\r\n      \"Learn how services interact with each other and why they are structured the way they are.\",\r\n  },\r\n};\r\n\r\n# System Services\r\n\r\nThese are services that are part of the TacoQ core system. The user doesn't\r\nuse these directly in their code and they exist purely to transport and store\r\ntask data.\r\n\r\n## Broker\r\n\r\n### Broker Responsabilities\r\n\r\nThe broker is responsible for transporting task objects between services. This\r\nworks by having a central exchange that routes **all task objects to the\r\nrelay** and new tasks to be executed to the **appropriate worker**.\r\n\r\n### RabbitMQ Implementation Details\r\n\r\nThe broker is implemented in RabbitMQ. Its structure is as follows:\r\n\r\n- The `relay_queue`, which is consumed by the Relay to continuously update the\r\n  task database. When a worker starts or finishes executing a task, they send\r\n  an update directly to this queue.\r\n- One queue per worker kind. More on worker kinds in the\r\n  [Worker](/technical-reference/system-architecture/#h2-worker) section.\r\n- A `task_exchange` exchange to which new tasks are published, being routed to\r\n  the appropriate worker queue based on their routing key (which is dictated\r\n  by the worker kind in the task object).\r\n\r\nQueues and exchanges are not customizable because RabbitMQ doesn't like it\r\nwhen different services declare different queue and exchange configurations\r\n(it crashes). Therefore:\r\n\r\n- All queues and exchanges are [durable](https://www.rabbitmq.com/docs/queues#durability) -\r\n  they will survive a RabbitMQ server restart.\r\n- All queues and exchanges are [do not auto-delete](https://www.rabbitmq.com/docs/queues#properties) -\r\n  they will not be deleted when the last consumer disconnects.\r\n- All queues have a default `{\"x-max-priority\": 255}` to allow for maximum\r\n  flexibility in [task priority](https://www.rabbitmq.com/docs/priority) and to\r\n  have the priority feature available by default.\r\n\r\n<Note>\r\n  We **do not plan on supporting additional brokers in the near future**, \r\n  but we are open to making the broker an abstract class and accepting \r\n  contributions for other message brokers if there is enough demand.\r\n\r\nThis is because we rely on RabbitMQ's routing and priorities features, which\r\nare not always present in other brokers.\r\n\r\n</Note>\r\n\r\n## Database\r\n\r\nThe latest state of each task is stored in the database. The database is\r\nimplemented in Postgres and managed via Rust's [sqlx](https://docs.rs/sqlx/latest/sqlx/)\r\nlibrary by the relay.\r\n\r\n<Note>\r\n  Some Postgres-backed task queues like [Hatchet](https://hatchet.run/) store \r\n  every event and use triggers to keep a materialized view with the latest state \r\n  of each task up to date.\r\n\r\nWe do not do this for a few reasons:\r\n\r\n- As the Hatchet team has noted, it is no easy feat to get the triggers\r\n  right.\r\n- Unlike Hatchet, we are only a task queue, not a workflow orchestrator -\r\n  it is not as important for us to store information about every step of a\r\n  workflow.\r\n- We already support OTEL tracing. We believe this is enough to get\r\n  observability into the system. The task also has information about the\r\n  timeline of its execution.\r\n</Note>\r\n\r\n<Note>\r\n  We use some Postgres-specific features like `LISTEN` and `NOTIFY` to implement\r\n  event-based task updates with the clients. Because of this, we do not plan on\r\n  suporting other databases in the near future.\r\n</Note>\r\n\r\n## Relay\r\n\r\nThe relay, as the name implies, is responsible for relaying information\r\nbetween the core services and the user's application. It is implemented in Rust\r\nand has the following capabilities:\r\n\r\n### 1. Task Update Consumer\r\n\r\nThe relay consumes task updates from the broker and stores them in the database as they come in.\r\n\r\n### 2. Data Retrieval\r\n\r\nThe relay also serves a **REST API** for retrieving task data from the database.\r\nYou can read the API swagger definition in [Relay Endpoints](/technical-reference/relay-endpoints).\r\nThe REST API is implemented in [Axum](https://docs.rs/axum/latest/axum/).\r\n\r\n### 3. Cleanup\r\n\r\nThe relay will run a job to delete tasks that have been in the database for\r\nlonger than a set period of time specified by the user. An index exists on the\r\nTTL column of the database to make this operation efficient.\r\n\r\n### 4. Replication\r\n\r\nThe relay is stateless and can be scaled horizontally if you need to\r\nload balance requests between multiple relays or increase the consuming\r\nrate of tasks.\r\n\r\n<Tip>\r\n  The relay has a lot of features packaged into a single service for the sake of simplicity.\r\n\r\nIf you only want to scale the consuming rate of tasks horizontally but you don't need more APIs,\r\nyou can use the environment variables `ENABLE_RELAY_TASK_CONSUMER`, `ENABLE_RELAY_CLEANUP` and\r\n`ENABLE_RELAY_API` to disable the features you don't need. Read more about environment variables\r\nin the [Relay environment variables](/technical-reference/relay-environment-variables) section.\r\n\r\n</Tip>\r\n\r\n# User Services\r\n\r\nThese are set up by the user himself and can safely go online and offline as\r\nneeded.\r\n\r\n## Worker\r\n\r\nThe worker is responsible for executing tasks. Each worker has a `worker_kind`\r\nand multiple `task_kind`'s that it is capable of executing.\r\n\r\n<Note>\r\n  Why are worker kinds a thing?\r\n\r\nIt is not uncommon for task queues to have all their workers consume from the\r\nsame queue. If you were to have two different workers with different task\r\ncapabilities, they would often consume tasks they are unable to execute, NACK\r\nthem, and send the task to the back of the queue. This could happen repeatedly\r\nand cause the task to never be executed, or at least be greatly delayed.\r\n\r\nAnother possible implementation would be to have one queue per task kind, which\r\nwould allow workers to only consume the queues they know they are able to\r\nexecute. This would, however, require the worker to have a strategy for\r\ndetermining which queue to prioritize consuming, increasing complexity and making\r\nthe behaviour more opaque.\r\n\r\nSo, we make the user explicitely decide which worker to route their task to,\r\nand we make the worker kind part of the task object.\r\n\r\nThe drawbacks to the current approach are:\r\n\r\n- Additional setup parameter that must be known pre-runtime.\r\n- There cannot be two different worker kinds with the shared task kind\r\n  capabilities. The user must always choose which worker kind to route a task to.\r\n\r\nGiven these extremely specific drawbacks which apply to almost no one and can\r\neasily be worked around, we've decided to use worker kinds to route tasks. If\r\nyou have a better idea, please let us know! :)\r\n\r\n</Note>\r\n\r\n## Publisher Client\r\n\r\nA task publisher client isn't a service but a client for submitting tasks to your\r\nworkers. It connects directly to the broker.\r\n\r\n## Relay Client\r\n\r\nAs task results are completed, they get stored in the database. To access them,\r\nan application must communicate with the relay via REST API. So, the client SDKs\r\nhave a **Relay Client** built in, whom is capable for retrieving task results via\r\nthe REST API.\r\n\r\n---\r\n\r\n# Appendix\r\n\r\n## Task & Event Objects\r\n\r\nServices communicate with each other by sending `Task`, `TaskAssignmentUpdate`, \r\n`TaskRunningUpdate`, and `TaskCompletedUpdate` objects. We split the updates\r\ninto different objects so we can be as compact as possible when sending small\r\nupdates.\r\n\r\nObjects are serialized and schematized using [Apache Avro](https://avro.apache.org/):\r\n\r\n- Schemata are defined in `schema/avro`\r\n- Correspondent idiomatic objects are defined in each SDK.\r\n- Avro helps us provide full backwards and forwards compatibility.\r\n\r\n\r\n\r\n",
          "headers": [
            {
              "title": "System Services",
              "type": "h1"
            },
            {
              "title": "Broker",
              "type": "h2"
            },
            {
              "title": "Broker Responsabilities",
              "type": "h3"
            },
            {
              "title": "RabbitMQ Implementation Details",
              "type": "h3"
            },
            {
              "title": "Database",
              "type": "h2"
            },
            {
              "title": "Relay",
              "type": "h2"
            },
            {
              "title": "1. Task Update Consumer",
              "type": "h3"
            },
            {
              "title": "2. Data Retrieval",
              "type": "h3"
            },
            {
              "title": "3. Cleanup",
              "type": "h3"
            },
            {
              "title": "4. Replication",
              "type": "h3"
            },
            {
              "title": "User Services",
              "type": "h1"
            },
            {
              "title": "Worker",
              "type": "h2"
            },
            {
              "title": "Publisher Client",
              "type": "h2"
            },
            {
              "title": "Relay Client",
              "type": "h2"
            },
            {
              "title": "Appendix",
              "type": "h1"
            },
            {
              "title": "Task & Event Objects",
              "type": "h2"
            }
          ]
        },
        {
          "url": "/technical-reference/versioning",
          "metadata": {
            "title": "Versioning",
            "description": "Learn how TacoQ handles versioning for images and libraries.",
            "icon": "SlidersHorizontal",
            "index": 2
          },
          "rawContent": "export const metadata = {\r\n  title: \"Versioning\",\r\n  description: \"Learn how TacoQ handles versioning for images and libraries.\",\r\n  keywords: [\r\n    \"Semantic Versioning\",\r\n    \"SemVer\",\r\n    \"version 1.0\",\r\n    \"major version\",\r\n    \"minor version\",\r\n    \"patch version\",\r\n    \"breaking changes\",\r\n    \"SDK versioning\",\r\n    \"image versioning\",\r\n    \"lockstep releases\",\r\n    \"Relay versioning\",\r\n    \"version compatibility\",\r\n    \"task object versioning\",\r\n    \"version upgrading\",\r\n    \"Avro\",\r\n    \"compatibility issues\",\r\n    \"NACK\",\r\n    \"worker compatibility\",\r\n    \"relay compatibility\",\r\n    \"version documentation\",\r\n    \"changelog\",\r\n  ],\r\n  openGraph: {\r\n    type: \"website\",\r\n    locale: \"en_US\",\r\n    siteName: \"TacoQ Docs\",\r\n    title: \"Versioning | TacoQ Docs\",\r\n    description: \"Learn how TacoQ handles versioning for images and libraries.\",\r\n  },\r\n};\r\n\r\n# Semantic Versioning\r\n\r\nUntil TacoQ reaches 1.0, it will adhere to the following versioning scheme:\r\n\r\n- The **major** version will stay at 0.\r\n- The **minor** version increases for breaking changes or major features.\r\n- The **patch** version increases for non-breaking changes.\r\n\r\nOnce we reach a stable 1.0, TacoQ will follow [Semantic versioning](https://semver.org/).\r\n\r\n# SDK and Image Lockstep Releases\r\n\r\nFor the time being, breaking changes in either the SDKs or the Relay image will\r\nresult in a new minor version - they are released in lockstep.\r\n\r\nPatch versions are not locked.\r\n\r\n# Task Object\r\n\r\nObjects passed along queues are versioned using [Apache Avro](https://avro.apache.org/).\r\nThey should always be backwards and forwards compatible, but we still recommend\r\nkeeping an eye on the [changelog](/changelog) for any breaking changes and keeping\r\nyour SDKs and the Relay image on the latest version.\r\n\r\n# Documentation\r\n\r\nDocumentation remains up to date and will not include versioning until 1.0. You\r\ncan always check the [changelog](/changelog) for information on breaking changes.\r\n",
          "headers": [
            {
              "title": "Semantic Versioning",
              "type": "h1"
            },
            {
              "title": "SDK and Image Lockstep Releases",
              "type": "h1"
            },
            {
              "title": "Task Object",
              "type": "h1"
            },
            {
              "title": "Documentation",
              "type": "h1"
            }
          ]
        },
        {
          "url": "/technical-reference/relay-environment-vars",
          "metadata": {
            "title": "Relay Configuration",
            "description": "Configure the relay using environment variables.",
            "icon": "Cog",
            "index": 3
          },
          "rawContent": "export const metadata = {\r\n  title: \"Relay Configuration\",\r\n  description:\r\n    \"Discover how to configure the relay using environment variables.\",\r\n  keywords: [\r\n    \"environment variables\",\r\n    \"relay configuration\",\r\n    \"broker connection\",\r\n    \"database connection\",\r\n    \"AMQP configuration\",\r\n    \"PostgreSQL configuration\",\r\n    \"OpenTelemetry config\",\r\n    \"telemetry setup\",\r\n    \"relay settings\",\r\n    \"connection URLs\",\r\n    \"functional configuration\",\r\n    \"task consumer settings\",\r\n    \"cleanup configuration\",\r\n    \"API configuration\",\r\n    \"OTEL configuration\",\r\n    \"service configuration\",\r\n    \"deployment settings\",\r\n    \"relay customization\",\r\n    \"tracing configuration\",\r\n    \"sampling configuration\"\r\n  ],\r\n  openGraph: {\r\n    type: \"website\",\r\n    locale: \"en_US\",\r\n    siteName: \"TacoQ Docs\",\r\n    title: \"Relay Configuration | TacoQ Docs\",\r\n    description:\r\n      \"Discover how to configure the relay using environment variables for connections, functionality, and telemetry.\",\r\n  },\r\n};\r\n\r\n# Configuration\r\n\r\nYou can configure the Relay by passing it environment variables. For example:\r\n\r\n```yaml {5-7}\r\nrelay:\r\n    image: ghcr.io/taco-xyz/tacoq-relay:latest\r\n    ports:\r\n      - \"3000:3000\"\r\n    environment:\r\n      TACOQ_DATABASE_URL: postgresql://user:password@localhost:5432/tacoq\r\n      TACOQ_BROKER_URL: amqp://user:password@localhost:5672\r\n```\r\n\r\n# Variables\r\n\r\n## Infrastructure Connections\r\n\r\nThe connections to the broker and the database can be configured using the following environment variables:\r\n\r\n- `TACOQ_BROKER_URL` - The URL of the broker. Example: `amqp://username:password@host:port/vhost`\r\n- `TACOQ_DATABASE_URL` - The URL of the database. Example: `postgresql://username:password@host:port/database_name`\r\n\r\n## Functional Decomposition\r\n\r\nThe relay serves multiple purposes and you might want to replicate only a \r\nsubset of its functionality. You can use the following environment variables to disable\r\nunwanted functionality:\r\n\r\n- `TACOQ_ENABLE_RELAY_TASK_CONSUMER` - Whether to enable the relay consuming the tasks from the broker. Default: `true`\r\n- `TACOQ_ENABLE_RELAY_CLEANUP` - Whether to enable the routine cleanup of expired tasks. Default: `true`\r\n- `TACOQ_ENABLE_RELAY_API` - Whether to enable the relay Axum API. Default: `true`\r\n\r\n## Telemetry\r\n\r\nThe telemetry configuration can be set using the following environment variables:\r\n\r\n- `OTEL_EXPORTER_OTLP_TRACES_ENDPOINT` - The endpoint for sending OpenTelemetry traces. Default: `http://localhost:4317`\r\n- `OTEL_EXPORTER_OTLP_TRACES_PROTOCOL` - The protocol for sending traces. Default: `grpc`\r\n- `OTEL_TRACES_SAMPLER` - The sampling strategy for traces. Default: `always_on`\r\n- `OTEL_SERVICE_NAME` - The service name for telemetry. Default: `tacoq.relay`\r\n",
          "headers": [
            {
              "title": "Configuration",
              "type": "h1"
            },
            {
              "title": "Variables",
              "type": "h1"
            },
            {
              "title": "Infrastructure Connections",
              "type": "h2"
            },
            {
              "title": "Functional Decomposition",
              "type": "h2"
            },
            {
              "title": "Telemetry",
              "type": "h2"
            }
          ]
        },
        {
          "url": "/technical-reference/benchmarks",
          "metadata": {
            "title": "Benchmarks",
            "description": "Learn how TacoQ compares to other task queues in terms of performance and scalability.",
            "icon": "ChartNoAxesCombined",
            "index": 4
          },
          "rawContent": "export const metadata = {\r\n  title: \"Benchmarks\",\r\n  description:\r\n    \"Learn how TacoQ compares to other task queues in terms of performance and scalability.\",\r\n  keywords: [\r\n    \"benchmarks\",\r\n    \"performance metrics\",\r\n    \"task queue comparison\",\r\n    \"scalability tests\",\r\n    \"performance testing\",\r\n    \"system performance\",\r\n    \"throughput\",\r\n    \"latency\",\r\n    \"comparative analysis\",\r\n    \"queue performance\",\r\n    \"system metrics\",\r\n    \"performance evaluation\",\r\n    \"task processing speed\",\r\n    \"scaling benchmarks\",\r\n  ],\r\n  openGraph: {\r\n    type: \"website\",\r\n    locale: \"en_US\",\r\n    siteName: \"TacoQ Docs\",\r\n    title: \"Benchmarks | TacoQ Docs\",\r\n    description:\r\n      \"Learn how TacoQ compares to other task queues in terms of performance and scalability.\",\r\n  },\r\n};\r\n\r\n<Warning>This section is under construction! Please check back soon :)</Warning>\r\n",
          "headers": []
        }
      ]
    },
    {
      "metadata": {
        "title": "Guides",
        "description": "Tutorials for common tasks using TacoQ.",
        "icon": "Map",
        "index": 2
      },
      "children": [
        {
          "url": "/guides/serialization",
          "metadata": {
            "title": "Serialization & Versioning",
            "description": "Optimize your task size and make them backwards and forwards compatible.",
            "icon": "Package",
            "index": 1
          },
          "rawContent": "# Motivation\r\n\r\nPassing data between services raises two big issues: performance and versioning:\r\n\r\n- **Performance**: The size of the data you are sending through the broker can\r\n  have a significant impact on the performance of your system.\r\n- **Versioning**: If you wish to change the schema of the input data of your \r\n  task, you need to ensure it is backwards compatible with the previous schema.\r\n\r\nProper serialization, clear schemas, and versioning strategies solve both problems.\r\n\r\n# Serialization Strategies\r\n\r\n## Simple Formats\r\n\r\n### JSON\r\n\r\nJSON is easy, fast to set up, and widely supported. If you're doing something\r\nsmall or medium-sized, it's a great choice.\r\n\r\n<Tip>\r\n  This can also be optimized through the use of [MessagePack](https://msgpack.org/),\r\n  which is a binary format that is more compact than JSON and faster to parse.\r\n</Tip>\r\n\r\n### CSV\r\n\r\nYou can also communicate information with CSV format, which is especially handy\r\nif you're working with data using something like `pandas` or `polars`.\r\n\r\n<Note>\r\n  [Polars](https://pola.rs/) is a fast, modern alternative to `pandas`. If you haven't already, we recommend giving it a try!\r\n</Note>\r\n\r\n### Simple Compatibility\r\n\r\nIf using JSON or CSV, you can still implement a rudimentary form of backwards\r\ncompatibility by assuming a default value for missing fields, ignoring\r\nfields that are no longer present in the schema, and, in the case of changed\r\nfield names, you can still read the old data by using the old name (you are\r\nimplementing a subset of Avro's schema evolution).\r\n\r\nForwards compatibility (reading data created by a newer version of the schema)\r\nis more complex to do manually. It can be done, but we recommend using a more\r\nserious serialization strategy for this.\r\n\r\n### Testing for Compatibility\r\n\r\nIt is not trivial to ensure that a JSON generated by `pydantic` can be parsed \r\nby a `serde` application.\r\n\r\nIt is a good idea to have **end-to-end** or **contract tests** that validate \r\nthat the publisher and consumer can communicate. If you have a lot of moving \r\nparts all making use of the same data, we recommend looking at a tool like \r\n[Pact](https://pact.io/) to ensure that the data is being serialized and\r\ndeserialized correctly.\r\n\r\n<Tip>\r\n  There also exist schema validation tools like [JSON Schema](https://json-schema.org/),\r\n  but if you are going through the trouble of using a schema, we recommend just\r\n  skipping to something like Protocol Buffers or Avro, which provide better\r\n  support for schema evolution and optimized encoding.\r\n</Tip>\r\n\r\n## Schema-Based Formats\r\n\r\n### Apache Thrift & Protocol Buffers\r\n\r\n[Apache Thrift](https://thrift.apache.org/) and [Protocol Buffers](https://protobuf.dev/)\r\nare binary encoding formats that are quite similar: they both require the user \r\nto define an interface definition language (IDL) that defines the schema of the\r\ndata:\r\n- Thrift uses `.thrift` files\r\n- Protocol Buffers use `.proto` files\r\n\r\nThese IDLs support the definition of data types, services, data evolution, and \r\nmore. You can then use them to generate code for the language you are using,\r\nand encode and decode them in a binary format.\r\n\r\nThese tools have obvious synergies with TacoQ due to having the guarantee that\r\nthe publisher and consumer are using the same schema while having idiomatic\r\ncode generation for the language you are using.\r\n\r\n<Note>\r\n  At Taco, we generally avoid code generation due to its complexity and \r\n  maintenance cost, though we won't dive into the full rationale here. If you\r\n  prefer a lower overhead approach, Avro is a great alternative.\r\n</Note>\r\n\r\n### Apache Avro\r\n\r\n[Apache Avro](https://avro.apache.org/) is a binary format that is similar to\r\nProtocol Buffers and Thrift, but its IDL is instead written using JSON. You can\r\nthen use primitive libraries like `fastavro` to encode and decode the data into\r\nyour own objects in that language.\r\n\r\nIt is lower overhead and does not require code-gen (though it exists!). It also\r\nsupports dynamically updating schemas, which means you can share the schema at\r\nruntime, and the consumer can still read the data, either via an endpoint or\r\nattaching the schema directly to the message.\r\n\r\n<Tip>\r\n  Check out our [Python](https://github.com/taco-xyz/tacoq/blob/main/client_sdks/python/tacoq/core/models/avro_serializable_base_model.py) \r\n  implementation. We believe it is simple and robust.\r\n</Tip>\r\n\r\n# Breaking Changes\r\n\r\nIt is likely that you will eventually need to make breaking changes to the \r\nschema that cannot be versioned. In this case, you should create an entirely\r\nnew `task_kind_v2`.\r\n\r\n<Note>\r\n  **Pattern: Task Redirection** \r\n  \r\n  For breaking schema changes, create a new task (e.g., `task_kind_v2`) like you\r\n  would with a REST endpoint. Refactor the old V1 task to only transform and forward\r\n  data to new tasks. \r\n  \r\n  This way, you know for certain that V1 is consuming V1 data, and V2 is \r\n  consuming V2 data, without having to pollute the code with both versions in\r\n  the same function.\r\n  \r\n  Remember that you now need to maintain two tasks. This can get very painful \r\n  very quickly, so you should deprecate it as soon as you can.\r\n</Note>\r\n\r\n# Claim Check Pattern\r\n\r\nWhen dealing with large payloads, it's important to remember that message \r\nbrokers are optimized for small, fast messages-not large data blobs. For large\r\npayloads, avoid message brokers. Instead, store data separately and reference \r\nit by ID. This is called the [Claim Check Pattern](https://www.enterpriseintegrationpatterns.com/patterns/messaging/StoreInLibrary.html).\r\n\r\n# Recommend Reading\r\n\r\nThis small guide can only do so much to explain the intricacies of \r\nserialization and versioning. If you want to do a deep dive into how to version\r\nyour tasks, we recommend the following resources:\r\n\r\n- [Designing Data-Intensive Applications](https://www.oreilly.com/library/view/designing-data-intensive-applications/9781491903063/), \r\n  Martin Kleppmann - **Chapter 4. Encoding and Evolution**\r\n- [Building Microservices](https://www.oreilly.com/library/view/building-microservices-2nd/9781492034018/), \r\n  Sam Newman - **Chapter 5. Implemeting Microservice Communication**\r\n\r\n<Tip>\r\n    These are also great books in general, and we recommend them wholeheartedly!\r\n</Tip>\r\n\r\n\r\n\r\n",
          "headers": [
            {
              "title": "Motivation",
              "type": "h1"
            },
            {
              "title": "Serialization Strategies",
              "type": "h1"
            },
            {
              "title": "Simple Formats",
              "type": "h2"
            },
            {
              "title": "JSON",
              "type": "h3"
            },
            {
              "title": "CSV",
              "type": "h3"
            },
            {
              "title": "Simple Compatibility",
              "type": "h3"
            },
            {
              "title": "Testing for Compatibility",
              "type": "h3"
            },
            {
              "title": "Schema-Based Formats",
              "type": "h2"
            },
            {
              "title": "Apache Thrift & Protocol Buffers",
              "type": "h3"
            },
            {
              "title": "Apache Avro",
              "type": "h3"
            },
            {
              "title": "Breaking Changes",
              "type": "h1"
            },
            {
              "title": "Claim Check Pattern",
              "type": "h1"
            },
            {
              "title": "Recommend Reading",
              "type": "h1"
            }
          ]
        },
        {
          "url": "/guides/same-app-worker-pattern",
          "metadata": {
            "title": "Same-app Worker",
            "description": "Set up a worker and publisher in the same application to make your life easier.",
            "icon": "Layers2",
            "index": 2
          },
          "rawContent": "# Hot Reloading\r\nYou can develop your worker with hot reloading by running it in the same\r\n\r\n# Implementation\r\n\r\nYou can set up the worker to run along with the rest of your application\r\nby running it in the background in an idiomatic way:\r\n\r\n```python\r\n# Set up your application as normal\r\nworker_app = WorkerApplication(...)\r\n\r\n# Instead of running the worker app via the CLI or a dedicated \r\n# entrypoint, run it the background as a co-routine.\r\nasyncio.create_task(worker_app.entrypoint())\r\n\r\n# When you are done using your worker, you can stop it \r\n# by calling the shutdown method.\r\nworker_app.shutdown()\r\n\r\n# If you want to wait for the worker to finish cleaning up the remaining \r\n# tasks, you can await the `wait_for_shutdown` method.\r\nawait worker_app.wait_for_shutdown()\r\n```\r\n\r\n<Warning>\r\n    This pattern is **not recommended for production use** because:\r\n    - It is important to separate the resources used by your worker and your \r\n      application.\r\n    - You cannot horizontally scale your worker by running multiple instances.\r\n    - Your worker may have blocking tasks that will interfere with your \r\n      application's performance.\r\n    \r\n    If you are using an asynchronous runtime like `tokio`, the impact of blocking\r\n    tasks may not be as pronounced due to the multiple threads available. In \r\n    `asyncio`, however, a blocking task in your worker will block the entire \r\n    process.\r\n</Warning>\r\n\r\n\r\n",
          "headers": [
            {
              "title": "Hot Reloading",
              "type": "h1"
            },
            {
              "title": "Implementation",
              "type": "h1"
            }
          ]
        }
      ]
    }
  ]
}
